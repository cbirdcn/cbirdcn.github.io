# 服务端重点-架构

## 端到端

### 架构设计与业务场景

[link](https://www.cnblogs.com/crazymakercircle/p/14367907.html)
[link](https://www.cnblogs.com/wpgraceii/p/10528183.html)

#### 系统设计

###### 秒杀

难点：并发量大，应用、数据库都承受不了。难控制超卖。

设计要点：

- 静态化：将请求尽量拦截在系统上游，html尽量静态化，部署到cdn上面。按钮及时设置为不可用，禁止用户重复提交请求。非覆盖式发布，静态资源必须共存多个静态版本。在 CDN 服务商提供的空间中，将回源地址配置为源站，而不是在代码中填写源站信息。
- 防止重复请求：设置页面缓存，针对同一个页面和uid一段时间内返回缓存页面。
- 访问缓存：数据用缓存抗，不直接落到数据库。
- 非强一致：读数据的时候不做强一致性校验，写数据的时候再做。
- 主机缓存：在每台物理机上也缓存商品详情、交易系统数据等等变动不大的相关的数据，避免网络交互耗时。
- 预加载缓存：像商品中的标题和描述这些本身不变的会在秒杀开始之前全量推送到秒杀机器上并一直缓存直到秒杀结束。
- 缓存随机失效：像库存这种动态数据会采用被动失效的方式缓存一定时间（一般是数秒），失效后再去Tair缓存拉取最新的数据。
- 异步防缓存穿透：如果允许的话，用异步的模式，等缓存都落库之后再返回结果。
- 流量削峰：请求排队、增加答题校验、逐层过滤无效请求、写操作限流抛弃系统承载能力外的请求等验证措施。

其他业务和技术保障措施：

- 业务隔离，预知规模。把秒杀做成一种营销活动，卖家要参加秒杀这种营销活动需要单独报名，从技术上来说，卖家报名后对我们来说就是已知热点，当真正开始时我们可以提前做好预热。
- 系统隔离，防雪崩。系统隔离更多是运行时的隔离，可以通过分组部署的方式和另外 99% 分开。秒杀还申请了单独的域名，目的也是让请求落到不同的集群中。从经济方面考虑，实在不能分开的系统就通过对服务器调用进行哈希分组，将热点请求分配到同一分组，防止对其他业务造成影响。
- 数据隔离，冷热分离。秒杀所调用的数据大部分都是热数据，比如会启用单独 cache 集群或 MySQL 数据库来放热点数据，目前也是不想0.01%的数据影响另外99.99%。

另外需要复习缓存穿透、雪崩等等问题，主要的流量都落在了缓存数据库上，需要针对缓存数据库的高可用作保障。

[如何设计一个秒杀系统08-答疑解惑：缓存失效的策略应该怎么定？](https://blog.csdn.net/sucaiwa/article/details/130758671)

[如何设计一个秒杀系统04-流量削峰这事应该怎么做？](https://blog.csdn.net/sucaiwa/article/details/130740127)

[系列文章](https://blog.csdn.net/sucaiwa/category_12312921.html)

###### 短链接（分布式ID生成，转字符串，存储）

比较公认的方案：

- 分布式ID生成器产生ID
- ID转62进制字符串
- 记录数据库，根据业务要求确定过期时间，可以保留部分永久链接
- 主要难点在于分布式ID生成。鉴于短链一般没有严格递增的需求，需要用雪花算法。否则如果按照机器数量预先分发一个号段，很难处理动态增减机器，以及容易被看出业务规模。

看了下新浪微博的短链接，8位，理论上可以保存超过200万亿对关系，具体怎么存储的还有待研究。

[分布式ID生成器之雪花算法简介](https://zhuanlan.zhihu.com/p/85837641)

[Leaf——美团点评分布式ID生成系统](https://tech.meituan.com/2017/04/21/mt-leaf.html)

![image](https://img-blog.csdnimg.cn/img_convert/8e2bd38320074336d4bc6aba7a7cf5d5.png)

雪花算法分布式ID组成部分（64bit）
- 第一位 占用1bit，其值始终是0，没有实际作用。 
- 时间戳 占用41bit，精确到毫秒，总共可以容纳约69年的时间。 
- 工作机器id 占用10bit，其中高位5bit是数据中心ID，低位5bit是工作节点ID，做多可以容纳1024个节点。 
- 序列号 占用12bit，每个节点每毫秒0开始不断累加，最多可以累加到4095，一共可以产生4096个ID。

SnowFlake算法在同一毫秒内最多可以生成多少个全局唯一ID呢：： 同一毫秒的ID数量 = 1024 X 4096 = 4194304

优点是不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，是递增且唯一的。缺点是，如果服务器出现时钟回拨，可以导致ID重复。

时间位可以是与某个指定时间点的时间戳差值，而不是时间戳的0值.

MongoDB的ObjectID也是通过"时间+机器码+pid+inc"共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。

对于worker_id的确定，可以借助通过 Zookeeper、Consul、Etcd 等提供分布式配置功能的中间件，也可以直接通过该机器的ip或本机配置获取worker id。

由于强依赖时钟，对时间的要求比较敏感，在机器工作时NTP同步也会造成秒级别的回退，建议可以直接关闭NTP同步。要么在时钟回拨的时候直接不提供服务直接返回ERROR_CODE，等时钟追上即可。或者做一层重试，然后上报报警系统，更或者是发现有时钟回拨之后自动摘除本身节点并报警。

实现关键代码

```Java
public class SnowFlakeService {
    //起始时间戳( 2020-12-26 00:00:00 )
    private static final long START_STAMP = 1608912000000L;

    //序列号最大值
    private static final long MAX_SEQUENCE = -1L ^ (-1L << 12); // 4095

    private static long sequence; //序列号  range(0 ~ 4095)

    public static synchronized long getNextId() {
        long currentStamp = System.currentTimeMillis();

        if (currentStamp < lastStamp) {
            throw new IllegalArgumentException("时间被回退，不能继续产生id");
        }

        if (currentStamp == lastStamp) {
            //相同毫秒内，序列号自增
            sequence = (sequence + 1) & MAX_SEQUENCE;
            if (sequence == 0L) {
                //序列号已经到最大值
                System.out.println("序列号已经到达最大值");
                //使用下一个时间戳(如果还没到下一时间戳，就陷入阻塞直至获得下一个时间戳才返回)
                currentStamp = getNextStamp();
            }
        } else {
            //不同毫秒，序列号重置
            sequence = 0L;
        }

        lastStamp = currentStamp;//当前时间戳存档，用于判断下次产生id时间戳是否相同

        return (currentStamp - START_STAMP) << TIMESTAMP_LEFT
                | dataCenterId << DATACENTER_LEFT
                | machineId << MACHINE_LEFT
                | sequence;
    }

    /**
     * 阻塞直至获得下一个时间戳
     * 注意：本机获取时间效率不够高，并且可能因为NTP导致时钟回拨出现错误。美团 Leaf-snowflake 方案引入zookeeper，每隔一段时间让本机上报时间，首先从zookeeper服务获取时间，如果发生回拨就拒绝服务或等待时钟恢复到递增，如果zookeeper服务异常才从本机获取时间。
     *
     * @return
     */
    public static long getNextStamp() {

        long newStamp = getCurrentStamp();
        while (newStamp <= lastStamp) {
            newStamp = getCurrentStamp();
        }

        return newStamp;
    }

    ...
}
```

###### 高并发红包系统（类似秒杀，数据库，减库存抢锁，冷热数据分离、请求排队）

微信红包（尤其是发在微信群里的红包，即群红包），业务形态上很类似网上的普通商品秒杀活动。

就像下面这样：
- 用户在微信群里发一个红包，等同于是普通商品秒杀活动的商品上架；
- 微信群里的所有用户抢红包的动作，等同于秒杀活动中的查询库存，减库存的时候会抢锁；
- 用户抢到红包后拆红包的动作，则对应秒杀活动中用户的秒杀动作。

不过除了上面的相同点之外，微信红包在业务形态上与普通商品秒杀活动相比，还具备自身的特点。

首先：**微信红包业务比普通商品秒杀有更海量的并发要求**。

微信红包用户在微信群里发一个红包，等同于在网上发布一次商品秒杀活动。假设同一时间有 10 万个群里的用户同时在发红包，那就相当于同一时间有 10 万个秒杀活动发布出去。10 万个微信群里的用户同时抢红包，将产生海量的并发请求。也就是说，同一个秒杀的总量不大，但是全局的并发量非常大，比如春晚可能几百万人同时抢红包。

其次：**微信红包业务要求更严格的安全级别**。

微信红包业务本质上是资金交易。微信红包是微信支付的一个商户，提供资金流转服务。

用户发红包时，相当于在微信红包这个商户上使用微信支付购买一笔钱，并且收货地址是微信群。当用户支付成功后，红包发货到微信群里，群里的用户拆开红包后，微信红包提供了将钱转入折红包用户微信零钱的服务。

资金交易业务比普通商品秒杀活动有更高的安全级别要求。普通的商品秒杀商品由商户提供，库存是商户预设的，秒杀时可以允许存在超卖（即实际被抢的商品数量比计划的库存多）、少卖（即实际被抢的商户数量比计划的库存少）的情况。但是对于微信红包，用户发 100 元的红包绝对不可以被拆出 101 元；用户发 100 元只被领取 99 元时，剩下的 1 元在 24 小时过期后要精确地退还给发红包用户，不能多也不能少。

以上是微信红包业务模型上的两大特点。

不能采用的方式：

- 乐观锁：DB 面临更大压力，以及事务回滚，所以不能采用。
- 直接用缓存顶，涉及到钱，一旦缓存挂掉就完了。

建议的方式：

- 接入层垂直切分，根据红包ID，发红包、抢红包、拆红包、查详情等等都在同一台机器上处理，互不影响，分而治之。
- 请求进行排队，到数据库的时候是串行的，就不涉及抢锁的问题了。
- 为了防止队列太长过载导致队列被降级，直接打到数据库上，所以数据库前面再加上一个缓存，用CAS自增控制并发，太高的并发直接返回失败。
- 红包冷热数据分离，按时间分表。
- 分布式id生成器（雪花，时间戳不能回拨）。

对 DB 的操作流程有以下三步，这三步操作需要在一个事务中完成：
- 锁库存；避免并发请求时出现超卖情况
- 插入秒杀记录；
- 更新库存。

设计难点就在这个事务操作上。商品库存在 DB 中记为一行，大量用户同时秒杀同一商品时，第一个到达 DB 的请求锁住了这行库存记录。在第一个事务完成提交之前这个锁一直被第一个请求占用，后面的所有请求需要排队等待。同时参与秒杀的用户越多，并发进 DB 的请求越多，请求排队越严重。因此，**并发请求抢锁，是典型的商品秒杀系统的设计难点**。

[社交软件红包技术解密(四) 微信红包系统是如何应对高并发的](http://www.52im.net/thread-2548-1-1.html)

[社交软件红包技术解密(一)：全面解密QQ红包技术方案——架构、技术实现等](http://www.52im.net/thread-2202-1-1.html)

在接入层，为了解决同时存在海量事务级操作的问题，将海量化为小量。可以
- 微信红包用户发一个红包时，微信红包系统生成一个 ID 作为这个红包的唯一标识。接下来这个红包的所有发红包、抢红包、拆红包、查询红包详情等操作，都根据这个 ID 关联。
- 红包系统根据这个红包 ID，按一定的规则哈希（如按 ID 尾号取模等），垂直上下切分。比如采用了一致性 Hash 寻址，保证同一个用户的请求只会落在同一台红包抽奖逻辑机器处理。切分后，一个垂直链条上的逻辑 Server 服务器、DB 统称为一个 SET。
- 各个 SET 之间相互独立，互相解耦。并且同一个红包 ID 的所有请求，包括发红包、抢红包、拆红包、查详情详情等，垂直 stick 到同一个 SET 内处理，高度内聚。通过这样的方式，系统将所有红包请求这个巨大的洪流分散为多股小流，互不影响，分而治之

在抽奖阶段，进行按设计合理的几率完成抽奖操作，将抽奖结果安全落地保存，并顺利发货等过程。面对海量抽奖请求，如何及时作出响应，是抽奖系统面临的难题。需要
- 在接入层采用一致性 Hash 算法：同一用户的抽奖请求只会转发到相同的抽奖系统处理;
- 抽奖系统采用缓存机制：在加快抽奖过程的同时也减少了对存储层的访问压力；
- 奖品配额机制：平滑抽奖过程，各类奖品按比例有序抽中；
- 流水和对账机制：保证抽奖数据最终无差错发放到用户账户中。

抽奖之前，需要先进行安全审计、账号鉴权、用户信息查询，然后执行抽奖逻辑后写入流水系统。流水系统和发货系统通过异步对账，保证库存变化和不会超卖。当抽奖模块请求发货系统给用户发货时，发货系统将执行发放过程。

另外，为了尽可能在抽奖前阶段缓存更多数据，而避免动态查询，可以将用户信息缓存在主机内存中，如果要从业务测改变中奖逻辑，还可以缓存用户中奖历史信息。

流水系统用于保存活动过程中的抽奖流水记录，可以在活动后对奖品发放和领用进行统计和对账。该系统还定时对领用失败的请求进行重做和对账，确保奖品发放到用户账户里。由于流水需要记录用户中奖的信息和领用的的情况，数据量巨大，所以抽奖逻辑层本地采用顺序写文件的方式进行记录。抽奖逻辑层会定期的把本地的流水文件同步到远程流水系统进行汇总和备份，同时，流水系统会对领用失败的流水进行重做，发送请求到抽奖逻辑层，抽奖逻辑层会调用发货系统的接口完成发货操作。

发货系统可能会对接各种不同的业务接口，为了避免发奖请求引起业务侧雪崩，就需要发货系统有队列延迟功能，可以暂存发奖信息，延迟发货。可以采用开源的 RocketMQ 消息中间件作为异步消息队列，暂存发货请求，再由礼券发货模块根据各业务的限速配置均匀地调用业务接口进行发货。

发货模块首先会到存储系统检查奖品是否真实有效，再到发货状态存储检查状态是否正常，只有真正需要的发货的奖品才向业务系统发起发货请求，确保发货的有效性，避免错发和多发。

由于采用异步发货，抽奖时刻奖品不能保证立即发放到用户账户中。但用户的奖品不会丢失，通过在异步队列中暂存，礼券发货模块逐步以合适的速度将奖品发放到用户账户中。如果发货过程中有延时或失败，用户可以通过多次领取提起发货请求，系统支持多次提交。如果多次发货仍然失败，对账工具第 2 天会从流水系统中将用户抽奖数据与发货数据进行对账，对发货异常用户再次发起发货。如果对账仍然失败，则提醒管理人员介入处理。

另外，在客户端也需要做一些工作
- 资源预加载。不经常变化的静态资源，如页面，图片，JS 等，会分发到各地 CDN 以提高访问速度，只有动态变化的内容，才实时从后台拉取。然而即使所有的静态资源都采用了 CDN 分发，如果按实际流量评估，CDN 的压力仍然无法绝对削峰。因为同时访问红包页面的人数比较多，按 83 万 / 秒的峰值，一个页面按 200K 评估，约需要 158.3G 的 CDN 带宽，会给 CDN 带来瞬间很大的压力。为减轻 CDN 压力，QQ 红包使用了手机 QQ 离线包机制提前把红包相关静态资源预加载到手机QQ移动端，这样可大大降低 CDN 压力。可以让将静态资源放入预加载列表，用户登录时分批次加载。也可以主动推送离线包，适合紧急更新，比如节日红包。
- 定时请求服务器避免频繁刷新。2.59 亿用户同时在线，用户刷一刷时的峰值高达 83 万 / 秒，用户每次刷的操作都向后台发起请求是没有必要的，因此手机 QQ 在移动端对用户刷一刷的操作进行计数，定时（1~3 秒）异步将汇总数据提交到后台抽奖，再将抽奖结果回传到手机 QQ 移动端显示。这样既保证了刷的畅快体验，也大大减轻后台压力，抽奖结果也在不经意间生产，用户体验完全无损。
- 错峰。对用户进行分组，不同组的用户刷一刷红包（企业明星红包、AR 红包等）的开始时间并不相同，而是错开一段时间（1~5 分钟）

###### 分布式限流器（滑动窗口计数器、令牌桶，redis+lua实现令牌桶）

[分布式服务限流实战](https://www.infoq.cn/article/qg2tx8fyw5vt-f3hh673)

常见的限流方法：

- 固定窗口计数器：按照时间段划分窗口，有一次请求就+1，最为简单的算法，但这个算法有时会让通过请求量允许为限制的两倍。
- 滑动窗口计数器：通过将窗口再细分，并且按照时间滑动来解决突破限制的问题，但是时间区间的精度越高，算法所需的空间容量就越大。
- 漏桶：请求类似水滴，先放到桶里，服务的提供方则按照固定的速率从桶里面取出请求并执行。缺陷也很明显，当短时间内有大量的突发请求时，即便此时服务器没有任何负载，每个请求也都得在队列中等待一段时间才能被响应。
- 令牌桶：往桶里面发放令牌，每个请求过来之后拿走一个令牌，然后只处理有令牌的请求。令牌桶满了则多余的令牌会直接丢弃。令牌桶算法既能够将所有的请求平均分布到时间区间内，又能接受服务器能够承受范围内的突发请求，因此是目前使用较为广泛的一种限流算法。

![令牌桶限流算法](https://static001.infoq.cn/resource/image/ec/93/eca0e5eaa35dac938c673fecf2ec9a93.png)

Google 的开源项目 guava 提供了 RateLimiter 类，实现了单点的令牌桶限流。

分布式环境下，可以考虑用 Redis+Lua 脚本实现令牌桶。

如果请求量太大了，Redis 也撑不住怎么办？我觉得可以类似于分布式 ID 的处理方式，Redis 前面在增加预处理，比如每台及其预先申请一部分令牌，只有令牌用完之后才去 Redis。如果还是太大，是否可以垂直切分？按照流量的来源，比如地理位置、IP 之类的再拆开。

###### 分布式定时任务（任务轮询+排队抢占）

任务轮询或任务轮询+抢占排队方案

- 每个服务器首次启动时加入队列；
- 每次任务运行首先判断自己是否是当前可运行任务，如果是便运行；
- 如果不是当前运行的任务，检查自己是否在队列中，如果在，便退出，如果不在队列中，进入队列。

###### 新浪微博推送机制（关系复杂、数据量大，推给在线粉丝，上线拉，冷热数据分离）

主要难点：关系复杂，数据量大。一个人可以关注非常多的用户，一个大 V 也有可能有几千万的粉丝。

先介绍最基本的方案：

- 推模式：推模式就是，用户A关注了用户 B，用户 B 每发送一个动态，后台遍历用户B的粉丝，往他们粉丝的 feed 里面推送一条动态。
- 拉模式：推模式相反，拉模式则是，用户每次刷新 feed 第一页，都去遍历关注的人，把最新的动态拉取回来。

一般采用推拉结合的方式，用户发送状态之后，先推送给粉丝里面在线的用户，然后不在线的那部分等到上线的时候再来拉取。

另外冷热数据分离，用户关系在缓存里面可以设置一个过期时间，比如七天。七天没上线的可能就很少用这个 APP。

###### 大文件有序内存排序（远高于内存，分隔，分别排序，缓冲区）

对于远高于内存的文件排序。

外归并排序：

- 对文件分割，然后分别排序。
- 排好序的文件依次读取一个缓冲区的大小，然后进行排序，输出到输出缓冲区，然后保存到结果文件。

如果是数字，可以用位图排序，但是要求比较苛刻：

- 数字不重复
- 知道最大值
- 相对密集，因为没出现的数字也会占用空间
-

比较适合电话号之类的。

[【算法】对一个20GB大的文件排序](https://blog.csdn.net/michellechouu/article/details/47002393)

#### 子业务场景

###### 外卖送单，保证只有一人接到（redis的lpush、rpop）

###### 加权推荐商家置顶，第二天更新（堆排序，第二天更新前redis准备好数据同步到db）

###### 微信抢红包（悲观锁、乐观锁、存储过程在mysql中）

###### 多任务分配有限人处理及优化（对象池、任务入N个队列，人去对应的队列取任务）

###### 保证发送消息的有序性（消息加header、识别header的syn）

###### 文件快速下发到服务（边下发边复制）

###### 多态机器存储大量日志，一台电脑选出热度最高的是个关键词（分bucket、分治法与多路归并）

###### 分布式集群如何保证线程安全（分布式锁，让分布式多线程对一个共享资源一次性只能有一个线程获取到锁里的资源，redis、zk居多）

#### 实现方式与概念

###### 负载均衡原理是什么?

负载均衡Load Balance）是高可用网络基础架构的关键组件，通常用于将工作负载分布到多个服务器来提高网站、应用、数据库或其他服务的性能和可靠性。负载均衡，其核心就是网络流量分发，分很多维度。

负载均衡是建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。

用户访问负载均衡器，再由负载均衡器将请求转发给后端服务器。在这种情况下，单点故障现在转移到负载均衡器上了。 这里又可以通过引入第二个负载均衡器来缓解。

负载均衡器如何选择要转发的后端服务器？

负载均衡器一般根据两个因素来决定要将请求转发到哪个服务器。首先，确保所选择的服务器能够对请求做出响应，然后根据预先配置的规则从健康服务器池（healthy pool）中进行选择。

因为，负载均衡器应当只选择能正常做出响应的后端服务器，因此就需要有一种判断后端服务器是否健康的方法。为了监视后台服务器的运行状况，运行状态检查服务会定期尝试使用转发规则定义的协议和端口去连接后端服务器。 如果，服务器无法通过健康检查，就会从池中剔除，保证流量不会被转发到该服务器，直到其再次通过健康检查为止。

负载均衡算法?

负载均衡算法决定了后端的哪些健康服务器会被选中。 其中常用的算法包括：

- Round Robin（轮询）：为第一个请求选择列表中的第一个服务器，然后按顺序向下移动列表直到结尾，然后循环。
- Least Connections（最小连接）：优先选择连接数最少的服务器，在普遍会话较长的情况下推荐使用。
- Source：根据请求源的 IP 的散列（hash）来选择要转发的服务器。这种方式可以一定程度上保证特定用户能连接到相同的服务器。
- 如果你的应用需要处理状态而要求用户能连接到和之前相同的服务器。可以通过 Source 算法基于客户端的 IP 信息创建关联，或者使用粘性会话（sticky sessions）。

除此之外，想要解决负载均衡器的单点故障问题，可以将第二个负载均衡器连接到第一个上，从而形成一个集群。

###### 滑动窗口的概念以及应用（流量控制技术，改善吞吐量，用在数据链路层、传输层）

###### 怎么做弹性扩缩容，原理是什么

###### 说一下中间件原理，设计

###### 设计一个web框架，你要怎么设计，说一下步骤

###### 怎么搞一个并发服务程序

###### 怎么做一个自动化配置平台系统

###### 各个系统出问题怎么监控报警

###### 怎么设计orm，让你写,你会怎么写

###### 电商交易订单系统

[数据冷热分离技术](https://cloud.tencent.com/developer/article/1798044)

在系统选型上，对于热数据系统，需要重点考虑读写的性能问题，诸如MySQL、Elasticsearch等会成为首选；而对于冷数据系统，则需要重点关注低成本存储问题，通常会选择存储在HDFS或云对象存储（比如AWS S3）中，再选择一个相应的查询系统。冷热数据是按照时间推移来区分的，因此必然要敲定一个时间分割线，即多久以内的数据为热数据，这个值通常会结合业务与历史访问情况来综合考量。对于超过时间线的数据，会被迁移到冷数据中，迁移过程需要确保两点：不能对热数据系统产生性能影响、不能影响数据查询。数据分离后，不可避免的会出现某个查询在时间上跨到两个系统里面，需要进行查询结果的合并，对于统计类查询就可能会出现一定的误差，需要在业务层面有所妥协。

业务背景是，用户在系统下单后会生成相应的交易订单信息，每天会产生大量的订单数据。这些数据需要永久保存，随时面对用户的低延迟查询，通常近3个月的订单是用户查询的主要对象。

在该系统中，热数据毫无疑问会采用MySQL(InnoDB)来实现，满足事务操作和高效查询的需求。当然，在查询系统前面还会有一层缓存，这里略过。冷数据以宽表的形式存储到HBase中，并采用Elasticsearch来提供相关的索引查询，配合HBase的数据查询。热数据在MySQL中保留90天，之后迁移到HBase中作为冷数据永久保存。

对于一个交易请求，会先在MySQL的订单表中创建订单记录，这些操作会通过BinLog同步到Kafka中，由Spark Streaming程序从Kafka中将相关订单信息变动提取出来，做相应的关联处理后写入到HBase中，同时更新Elasticsearch中的索引信息。每天定期将冷热分割线往前推移，并删除热数据中对应时间的订单表。

###### 验证码

#### 思路

从以下几个方面准备：

- 先了解常用算法，针对解决各种问题能用哪些算法，比如大文件排序用外排序，大量数据中的命中判断用位图/布隆过滤器等等。
- 注意扩展性、多考虑极端情况，多问自己几个为什么。比如说起单机的限流算法想想分布式的怎么做。
- 实在不知道怎么弄的叙述自己的思考过程，着重展示自己考虑周全、思维缜密。
